{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from copy import deepcopy as dp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from copy import deepcopy\n",
    "import multiprocessing\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.548577e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Hello,\\n\\n&amp;amp;#x200B;\\n\\nI'll be flying into ...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Question regarding customs when arriving in India</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/akaal5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.548603e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi, I am looking to invest a part of my monthl...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Need Advice about Flexi RD</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/akcwld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.547265e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>After Aquaman's success, I saw Indians on Twit...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Why do a disproportionate amount Indians have ...</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/af3mlj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.547329e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Any stories?</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>How much do you listen to your parents’ advice...</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/afbzhn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.546750e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Many friends and relatives of mine who have wo...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>A question regarding on-site opportunities (or...</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ad2gkb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  link_flair_text  num_comments  score  \\\n",
       "0  1.548577e+09                3             4      0   \n",
       "1  1.548603e+09                3             1      0   \n",
       "2  1.547265e+09                3            15      0   \n",
       "3  1.547329e+09                3             0      0   \n",
       "4  1.546750e+09                3             4      0   \n",
       "\n",
       "                                            selftext subreddit_id  \\\n",
       "0  Hello,\\n\\n&amp;#x200B;\\n\\nI'll be flying into ...     t5_2qh1q   \n",
       "1  Hi, I am looking to invest a part of my monthl...     t5_2qh1q   \n",
       "2  After Aquaman's success, I saw Indians on Twit...     t5_2qh1q   \n",
       "3                                       Any stories?     t5_2qh1q   \n",
       "4  Many friends and relatives of mine who have wo...     t5_2qh1q   \n",
       "\n",
       "                                               title  \\\n",
       "0  Question regarding customs when arriving in India   \n",
       "1                         Need Advice about Flexi RD   \n",
       "2  Why do a disproportionate amount Indians have ...   \n",
       "3  How much do you listen to your parents’ advice...   \n",
       "4  A question regarding on-site opportunities (or...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/india/comments/akaal5...  \n",
       "1  https://www.reddit.com/r/india/comments/akcwld...  \n",
       "2  https://www.reddit.com/r/india/comments/af3mlj...  \n",
       "3  https://www.reddit.com/r/india/comments/afbzhn...  \n",
       "4  https://www.reddit.com/r/india/comments/ad2gkb...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=df1['title'].tolist()\n",
    "body=df1['selftext'].tolist()\n",
    "url=df1['url'].tolist()\n",
    "y=df1['link_flair_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stopwords_removal(tokens):\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words]              \n",
    "    final=' '.join(filtered_sentence)\n",
    "    return final\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    obj=str.maketrans('', '',string.punctuation)\n",
    "    answer=text.translate(obj)\n",
    "    return answer\n",
    "\n",
    "def simple_lemma(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered=[lemmatizer.lemmatize(word) for word in data.split()]\n",
    "    text = ' '.join(filtered)\n",
    "    return text\n",
    "\n",
    "def pre_process(y,flag):\n",
    "    x=deepcopy(y)\n",
    "    for i in range(len(x)):\n",
    "        if x[i]!='nan':\n",
    "            if flag:\n",
    "                x[i]=x[i].split('/')\n",
    "                x[i]=' '.join(x[i])\n",
    "            x[i]=simple_lemma(remove_punctuation(stopwords_removal(word_tokenize(str(x[i]).lower()))))\n",
    "        else:\n",
    "            x[i]=''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[0 for _ in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df1['link_flair_text'].tolist():\n",
    "    c[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[246, 6026, 1176, 10798, 1169, 1205, 171, 430, 305, 507, 210]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_clean=pre_process(title,False)\n",
    "body_clean=pre_process(body,False)\n",
    "url_clean=pre_process(url,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Question regarding customs when arriving in India',\n",
       " 'question regarding custom arriving india')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[0],title_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Hello everyone,\\n\\nI've got my first job and its the contract ending with the ending of this month. I'd gone through a real emotional turmoil during those 70+ odd interviews.\\n\\nThough I've made myself content by started to read game of thrones books and most of the time quora and reddit.\\n\\nMy hobbies are acting dramatics and i do like to write stories.\\n\\nAbout you, you should be self motivated about your career and relations. I would not want someone who just like those fake or small talks and then vanish away.\\n\\nIf it turns out good I'd definitely marry. I'm not someone who is just lurking around for sexual favours or something. I appreciate the thoughts and the importance it has while falling for love.\\n\\nso looking forward keep talking in chats and then if you want I can move near you as well. I already have interviews for job opportunities in Germany, sweeden, Netherlands, Singapore and mighty USA. ( Since EU countries allows developers to get there) \\n\\nSo I want to see how much our frequency matches.\\n\\nMy soul purpose of putting this post here to get some feedback on myself. Since I do hangout with people from office have some drinks with them but then I mostly found is they ignore me for even Lunch or tea break times. \\n\\nI have started understanding the whole Football world by following those Premier League, UEFA, LaLiga, that is going on right now thank to reddit. And putting all that effort just so that I can converse and make more friends with people who are deeply involved in Football world. But still its the same people just ignore or forget about me.\\n\\nThe thing which annoys me the most is I can't keep doing small talk for everyday for 2-3 months I want someone reliable to be my friend and keep talking to me, like I have this concept of being best friend you do remember the past talks otherwise its like talking to a vending machine. \\n\\nThe thing that saddens me the most is I see a lot of nay-sayers, shallow, less-knowledgable persons getting awesome jobs and girls but when I see myself that I'm doing all those work and not achieving I feel so much sad and I just look down on myself so much. I'm not some creep or introvert who can't make conversations I do try it and hold the conversation but why god why? Like if you want to talk about cosmetics I would like to talk about what Revelon is doing with their products etc.\\n\\nSo all my fellow Indians here please guide me as I'm feeling so out of life and my path in terms of career and personal life as well. Mom at back home is looking for girls to directly marry me, I'm not saying I don't want to marry but then they control that I can't talk to a girl who is like 4-5 years elder to me so I have a lot of BS to tackle.\\n\\nI'm yet to understand all those financial taxation and how will I invest to have a good savings in coming years I also need to help my dad with that house loan but not much burden is there.\\n\\nWill life be like this all the time in India where you are just moving on roads with crowd hustling for going to job and come back to cook on my own ( that too sometimes backfire and make a messy cooker :D) I got really afraid after I have seen recent interview of Anurag Kashyap saying that in India we are just barely able to survive. I don't want to digest that but I know that's the truth. And even this tech giant metro city I feel all alone every day after coming back from office and after cooking. \\n\\nPS.: I'm not some psycho or sex maniac so please don't troll me over it.\",\n",
       " 'hello everyone ve got first job contract ending ending month d gone real emotional turmoil 70 odd interview though ve made content started read game throne book time quora reddit hobby acting dramatics like write story self motivated career relation would want someone like fake small talk vanish away turn good d definitely marry m someone lurking around sexual favour something appreciate thought importance falling love looking forward keep talking chat want move near well already interview job opportunity germany sweeden netherlands singapore mighty usa since eu country allows developer get want see much frequency match soul purpose putting post get feedback since hangout people office drink mostly found ignore even lunch tea break time started understanding whole football world following premier league uefa laliga going right thank reddit putting effort converse make friend people deeply involved football world still people ignore forget thing annoys ca nt keep small talk everyday 23 month want someone reliable friend keep talking like concept best friend remember past talk otherwise like talking vending machine thing saddens see lot naysayer shallow lessknowledgable person getting awesome job girl see m work achieving feel much sad look much m creep introvert ca nt make conversation try hold conversation god like want talk cosmetic would like talk revelon product etc fellow indian please guide m feeling life path term career personal life well mom back home looking girl directly marry m saying nt want marry control ca nt talk girl like 45 year elder lot b tackle m yet understand financial taxation invest good saving coming year also need help dad house loan much burden life like time india moving road crowd hustling going job come back cook sometimes backfire make messy cooker got really afraid seen recent interview anurag kashyap saying india barely able survive nt want digest know s truth even tech giant metro city feel alone every day coming back office cooking p m psycho sex maniac please nt troll')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body[5],body_clean[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=10000, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ti_bo_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo_ur.append(title_clean[i]+' '+body_clean[i]+' '+url_clean[i])\n",
    "tfidf= TfidfVectorizer(max_features=10000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "tfidf.fit(ti_bo_ur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier,X_train,y_train,X_test,y_test):\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_train_pred=classifier.predict(X_train)\n",
    "    y_test_pred=classifier.predict(X_test)\n",
    "    return accuracy_score(y_train,y_train_pred),accuracy_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst=(np.random.rand(len(df1)) < 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test(x):\n",
    "    X_train=[]\n",
    "    X_test=[]\n",
    "    y1_train=[]\n",
    "    y1_test=[]\n",
    "    for i in range(len(mst)):\n",
    "        if mst[i]:\n",
    "            X_train.append(x[i])\n",
    "            y1_train.append(y[i])\n",
    "        else:\n",
    "            X_test.append(x[i])\n",
    "            y1_test.append(y[i])\n",
    "    \n",
    "    X1_train=tfidf.transform(X_train)\n",
    "    X1_test=tfidf.transform(X_test)\n",
    "    \n",
    "    \n",
    "    clf1=GridSearchCV(SVC(gamma='scale'),{'decision_function_shape':['ovo','ovr'],},cv=5,iid=False)\n",
    "    print(train(clf1,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf2=GridSearchCV(KNeighborsClassifier(),{'n_neighbors':[2,5,7]},cv=5,iid=False)\n",
    "    print(train(clf2,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf3=GridSearchCV(DecisionTreeClassifier(random_state=0),{'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf3,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf4=GridSearchCV(LogisticRegression(random_state=0),{'solver':['newton-cg', 'lbfgs','sag', 'saga','lbfgs'],'multi_class':['multinomial'],'max_iter':[200,250,300]},cv=5,iid=False)\n",
    "    print(train(clf4,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf5=GridSearchCV(RandomForestClassifier(random_state=0),{'n_estimators':[10,50,100,150],'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf5,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf6=GridSearchCV(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42),{'max_depth':[2,3],'learning_rate':[0.1,0.15,0.2]},cv=5,iid=False)\n",
    "    print(train(clf6,X1_train,y1_train,X1_test,y1_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8795975405254332, 0.6868826096944636)\n",
      "(0.8394633873672442, 0.518263266712612)\n",
      "(0.9890441587479039, 0.6381805651274983)\n",
      "(0.7376187814421464, 0.6453020905122904)\n",
      "(0.9890441587479039, 0.6829772570640937)\n",
      "(0.6195081050866406, 0.5860326211807948)\n"
     ]
    }
   ],
   "source": [
    "train_test(title_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6880939072107323, 0.6666666666666666)\n",
      "(0.6577417551704863, 0.6416264645072364)\n",
      "(0.7048071548351034, 0.6620721341603492)\n",
      "(0.6798211291224148, 0.6650585802894555)\n"
     ]
    }
   ],
   "source": [
    "train_test(url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test(body_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_bo=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo.append(title_clean[i]+\" \"+body_clean[i])\n",
    "train_test(ti_bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test(ti_bo_ur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_ur=[]\n",
    "for i in range(len(body_clean)):\n",
    "    bo_ur.append(body_clean[i]+\" \"+url_clean[i])\n",
    "train_test(bo_ur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_ur.append(title_clean[i]+\" \"+url_clean[i])\n",
    "train_test(ti_ur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42),{'max_depth':[3],'learning_rate':[0.1,0.2]},cv=5,iid=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidfconverter.fit_transform(ti_bo_ur).toarray()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=42, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'max_depth': [3], 'learning_rate': [0.1, 0.2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147883452446399\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(X)\n",
    "print(accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ti_bo_ur' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-afc4554502a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti_bo_ur\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_cleaned2.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ti_bo_ur' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(ti_bo_ur,columns=['text']).to_csv('data_cleaned2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf','wb') as f:\n",
    "    pickle.dump(tfidf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2=pickle.load(open('tfidf','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
