{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from copy import deepcopy as dp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rahul Gandhi resigns as President of Congress</td>\n",
       "      <td>1120</td>\n",
       "      <td>c8lydy</td>\n",
       "      <td>https://www.aninews.in/news/national/general-n...</td>\n",
       "      <td>413</td>\n",
       "      <td>1.562175e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is legalization of marijuana in India ever goi...</td>\n",
       "      <td>81</td>\n",
       "      <td>bvvre5</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/bvvre5...</td>\n",
       "      <td>124</td>\n",
       "      <td>1.559497e+09</td>\n",
       "      <td>As a cannabis enthusiast (stoner, pothead), it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was searching for something and I stumbled u...</td>\n",
       "      <td>314</td>\n",
       "      <td>cd3xmx</td>\n",
       "      <td>https://i.imgur.com/FefhpHc.jpg</td>\n",
       "      <td>109</td>\n",
       "      <td>1.563145e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emperors of Extraction: The Mughals did not ma...</td>\n",
       "      <td>35</td>\n",
       "      <td>cdargd</td>\n",
       "      <td>https://www.dailyo.in/politics/emperors-of-ext...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.563181e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chennaite here. Is there any way for us to mak...</td>\n",
       "      <td>83</td>\n",
       "      <td>c22c7k</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/c22c7k...</td>\n",
       "      <td>30</td>\n",
       "      <td>1.560894e+09</td>\n",
       "      <td>It's crazy how they aren't even talking about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0      Rahul Gandhi resigns as President of Congress   1120  c8lydy   \n",
       "1  Is legalization of marijuana in India ever goi...     81  bvvre5   \n",
       "2  I was searching for something and I stumbled u...    314  cd3xmx   \n",
       "3  Emperors of Extraction: The Mughals did not ma...     35  cdargd   \n",
       "4  Chennaite here. Is there any way for us to mak...     83  c22c7k   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0  https://www.aninews.in/news/national/general-n...        413  1.562175e+09   \n",
       "1  https://www.reddit.com/r/india/comments/bvvre5...        124  1.559497e+09   \n",
       "2                    https://i.imgur.com/FefhpHc.jpg        109  1.563145e+09   \n",
       "3  https://www.dailyo.in/politics/emperors-of-ext...          7  1.563181e+09   \n",
       "4  https://www.reddit.com/r/india/comments/c22c7k...         30  1.560894e+09   \n",
       "\n",
       "                                                body  flair  \n",
       "0                                                NaN      0  \n",
       "1  As a cannabis enthusiast (stoner, pothead), it...      0  \n",
       "2                                                NaN      0  \n",
       "3                                                NaN      0  \n",
       "4  It's crazy how they aren't even talking about ...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=df1['title'].tolist()\n",
    "body=df1['body'].tolist()\n",
    "url=df1['url'].tolist()\n",
    "y=df1['flair'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stopwords_removal(tokens):\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words]              \n",
    "    final=' '.join(filtered_sentence)\n",
    "    return final\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    obj=str.maketrans('', '',string.punctuation)\n",
    "    answer=text.translate(obj)\n",
    "    return answer\n",
    "\n",
    "def simple_lemma(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered=[lemmatizer.lemmatize(word) for word in data.split()]\n",
    "    text = ' '.join(filtered)\n",
    "    return text\n",
    "\n",
    "def pre_process(y,flag):\n",
    "    x=deepcopy(y)\n",
    "    for i in range(len(x)):\n",
    "        if str([i])!='nan':\n",
    "            if flag:\n",
    "                x[i]=x[i].split('/')\n",
    "                x[i]=' '.join(x[i])\n",
    "            x[i]=simple_lemma(remove_punctuation(stopwords_removal(word_tokenize(str(x[i]).lower()))))\n",
    "        else:\n",
    "            x[i]=''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rahul Gandhi resigns as President of Congress',\n",
       " 'Is legalization of marijuana in India ever going to be a serious political issue during our lifetime, or are we going to be criminals forever?',\n",
       " 'I was searching for something and I stumbled upon my 2nd std papers! 9Â¾/10 bois, AMA ðŸ˜‚',\n",
       " 'Emperors of Extraction: The Mughals did not make India rich. Claims of their welfarism only buttress a political agenda',\n",
       " 'Chennaite here. Is there any way for us to make the political parties talk about building ponds and reservoirs?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_clean=pre_process(title,False)\n",
    "body_clean=pre_process(body,False)\n",
    "url_clean=pre_process(url,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'cannabis enthusiast stoner pothead pain see many country across world taking issue seriously re reduced criminal status purchasing illegal weed questionable source expect india take debate anytime soon',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 's crazy nt even talking even opposition ca nt go like p situation grim chennai somehow sound worse medium guess could say dire situation nt case chennai depends locality ground water run almost run locality every year situation gradually worsening guess']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_clean[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier,X_train,y_train,X_test,y_test):\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_train_pred=classifier.predict(X_train)\n",
    "    y_test_pred=classifier.predict(X_test)\n",
    "    return accuracy_score(y_train,y_train_pred),accuracy_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst=(np.random.rand(len(df1)) < 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test(x):\n",
    "    X_train=[]\n",
    "    X_test=[]\n",
    "    y1_train=[]\n",
    "    y1_test=[]\n",
    "    for i in range(len(mst)):\n",
    "        if mst[i]:\n",
    "            X_train.append(x[i])\n",
    "            y1_train.append(y[i])\n",
    "        else:\n",
    "            X_test.append(x[i])\n",
    "            y1_test.append(y[i])\n",
    "    tf = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "    X1_train = tf.fit_transform(X_train)  \n",
    "    X1_test = tf.transform(X_test)\n",
    "    \n",
    "    clf1=GridSearchCV(SVC(gamma='scale'),{'decision_function_shape':['ovo','ovr'],},cv=5,iid=False)\n",
    "    print(train(clf1,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf2=GridSearchCV(KNeighborsClassifier(),{'n_neighbors':[2,5,7]},cv=5,iid=False)\n",
    "    print(train(clf2,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf3=GridSearchCV(DecisionTreeClassifier(random_state=0),{'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf3,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf4=GridSearchCV(LogisticRegression(random_state=0),{'solver':['newton-cg', 'lbfgs','sag', 'saga','lbfgs'],'multi_class':['multinomial'],'max_iter':[200,250,300]},cv=5,iid=False)\n",
    "    print(train(clf4,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf5=GridSearchCV(RandomForestClassifier(random_state=0),{'n_estimators':[10,50,100,150],'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf5,X1_train,y1_train,X1_test,y1_test))\n",
    "    \n",
    "    clf6=GridSearchCV(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42),{'max_depth':[2,3],'learning_rate':[0.1,0.15,0.2]},cv=5,iid=False)\n",
    "    print(train(clf6,X1_train,y1_train,X1_test,y1_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8954293628808865, 0.5653333333333334)\n",
      "(0.5290858725761773, 0.3253333333333333)\n",
      "(0.9342105263157895, 0.5173333333333333)\n",
      "(0.7970914127423823, 0.5733333333333334)\n",
      "(0.9342105263157895, 0.5706666666666667)\n",
      "(0.7673130193905817, 0.5493333333333333)\n"
     ]
    }
   ],
   "source": [
    "train_test(title_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.31163434903047094, 0.25866666666666666)\n",
      "(0.2236842105263158, 0.22133333333333333)\n",
      "(0.31301939058171746, 0.2613333333333333)\n",
      "(0.29709141274238227, 0.26666666666666666)\n",
      "(0.31301939058171746, 0.2693333333333333)\n",
      "(0.30055401662049863, 0.26666666666666666)\n"
     ]
    }
   ],
   "source": [
    "train_test(url_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7728531855955678, 0.4613333333333333)\n",
      "(0.24376731301939059, 0.14133333333333334)\n",
      "(0.7811634349030471, 0.536)\n",
      "(0.7049861495844876, 0.488)\n",
      "(0.7811634349030471, 0.5653333333333334)\n",
      "(0.7652354570637119, 0.592)\n"
     ]
    }
   ],
   "source": [
    "train_test(body_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9480609418282548, 0.6906666666666667)\n",
      "(0.2610803324099723, 0.17333333333333334)\n",
      "(0.9605263157894737, 0.8106666666666666)\n",
      "(0.8975069252077562, 0.728)\n",
      "(0.9605263157894737, 0.8133333333333334)\n",
      "(0.9349030470914127, 0.848)\n"
     ]
    }
   ],
   "source": [
    "ti_bo=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo.append(title_clean[i]+\" \"+body_clean[i])\n",
    "train_test(ti_bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9501385041551247, 0.7066666666666667)\n",
      "(0.7084487534626038, 0.5226666666666666)\n",
      "(0.9626038781163435, 0.824)\n",
      "(0.9002770083102493, 0.7413333333333333)\n",
      "(0.9626038781163435, 0.8186666666666667)\n",
      "(0.9224376731301939, 0.8506666666666667)\n"
     ]
    }
   ],
   "source": [
    "ti_bo_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo_ur.append(title_clean[i]+\" \"+body_clean[i]+\" \"+url_clean[i])\n",
    "train_test(ti_bo_ur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.860803324099723, 0.544)\n",
      "(0.2756232686980609, 0.144)\n",
      "(0.8760387811634349, 0.6186666666666667)\n",
      "(0.7742382271468145, 0.5413333333333333)\n",
      "(0.8760387811634349, 0.6266666666666667)\n",
      "(0.8081717451523546, 0.6426666666666667)\n"
     ]
    }
   ],
   "source": [
    "bo_ur=[]\n",
    "for i in range(len(body_clean)):\n",
    "    bo_ur.append(body_clean[i]+\" \"+url_clean[i])\n",
    "train_test(bo_ur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9058171745152355, 0.576)\n",
      "(0.48753462603878117, 0.28)\n",
      "(0.9439058171745153, 0.5413333333333333)\n",
      "(0.8130193905817175, 0.592)\n",
      "(0.9439058171745153, 0.576)\n",
      "(0.7617728531855956, 0.6026666666666667)\n"
     ]
    }
   ],
   "source": [
    "ti_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_ur.append(title_clean[i]+\" \"+url_clean[i])\n",
    "train_test(ti_ur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42),{'max_depth':[2,3],'learning_rate':[0.1,0.15,0.2]},cv=5,iid=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidfconverter.fit_transform(ti_bo_ur).toarray()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=42, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'max_depth': [2, 3], 'learning_rate': [0.1, 0.15, 0.2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9263331500824629\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(X)\n",
    "print(accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ti_bo_ur,columns=['text']).to_csv('data_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
