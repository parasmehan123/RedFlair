{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/parasmehan123/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from sklearn.externals import joblib\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "import praw\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7,stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=1500, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fit(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "def clean(X):\n",
    "    clean_X = []\n",
    "\n",
    "    for sen in X:  \n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(sen))\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        clean_X.append(document)\n",
    "    return clean_X\n",
    "\n",
    "def clean_url(url):   \n",
    "    url_clean = []\n",
    "    for sen in url:\n",
    "        sen=sen.replace('/',' ')\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(sen))\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "        if document==\"nan\":\n",
    "            url_clean.append('')\n",
    "        else:\n",
    "            url_clean.append(document)\n",
    "    return url_clean\n",
    "def clean_master(ti,bo,ur):\n",
    "    ti_clean=clean(ti)\n",
    "    bo_clean=clean(bo)\n",
    "    ur_clean=clean_url(ur)\n",
    "    text=[]\n",
    "    for i in range(len(ti)):\n",
    "        text.append(ti_clean[i]+' '+bo_clean[i]+' '+ur_clean[i])\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_dict={0:'Political',1:'Non-political',2:'Reddiquette',3:'AskIndia',4:'Science & Technology',5:'Policy & Economy',6:'Finance & Business',7:'Sports and food',8:'Photography',9:'AMA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flair(u):\n",
    "    reddit=praw.Reddit(client_id='ST0obmq3HAomHQ',client_secret='ECeazJYbpZ5Kx83g6RrWPDyPO0A',user_agent='Precog Project',username='parasmehan123',password='pMarsMehan@123')\n",
    "    s=reddit.submission(url=u)\n",
    "    bo=[s.selftext]\n",
    "    ti=[s.title]\n",
    "    ur=[s.url]\n",
    "    text=clean_master(ti,bo,ur)\n",
    "    print(text)\n",
    "    return flair_dict[clf.predict(tf.transform(text))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunil chhetri  http i redd it u3fc46hs8la31 jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Political'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flair(\"https://www.reddit.com/r/india/comments/cdryvu/sunil_chhetri/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lpt amazon flipkart sale it never a good deal if you were not going to buy that item in the first place if you don t need an item and you are buying it solely for the reason that it is on sale then that wasn t really a good financial decision if you are cautious about your finance if you really need an item then buying it without that item being on sale is still bang for your buck http www reddit com r india comment cdqtei lpt_amazonflipkart_sale_its_never_a_good_deal_if']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AMA'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flair(\"https://www.reddit.com/r/india/comments/cdqtei/lpt_amazonflipkart_sale_its_never_a_good_deal_if/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how an indian tycoon fought big pharma to sell aid drug for 1 a day  http qz com india 1666032 how indian pharma giant cipla made aid drug affordable']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AMA'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flair(\"https://www.reddit.com/r/india/comments/cdkoc8/how_an_indian_tycoon_fought_big_pharma_to_sell/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['psa change region in your xiaomi phone to any country in europe to get rid of all the bloatware all the shitty apps will disappear they are not uninstalled i believe but the apps don t clutter your menu and there are no notification something is better nothing if you don t want to root your device http www reddit com r india comment cdrwii psa_change_region_in_your_xiaomi_phone_to_any']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AMA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flair(\"https://www.reddit.com/r/india/comments/cdrwii/psa_change_region_in_your_xiaomi_phone_to_any/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mahua moitra file criminal defamation case against sudhir chaudhary  http thewire in law mahua moitra criminal defamation sudhir chaudhary']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Photography'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flair(\"https://www.reddit.com/r/india/comments/cdifvp/mahua_moitra_files_criminal_defamation_case/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl=pd.read_csv('data.csv')[\"flair\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=tf.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549202858713579"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf.predict(text),fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
